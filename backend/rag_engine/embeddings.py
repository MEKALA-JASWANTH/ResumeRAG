"""\nEmbedding Service - Converts text to vector embeddings\nSupports multiple embedding models for semantic search\n"""\n\nfrom sentence_transformers import SentenceTransformer\nfrom typing import List, Optional\nimport logging\nimport numpy as np\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingService:\n    """\n    Handles text embedding generation using sentence-transformers\n    """\n    \n    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):\n        """\n        Initialize the embedding model\n        \n        Args:\n            model_name: Name of the sentence-transformer model\n                       Default: all-MiniLM-L6-v2 (fast, good for general use)\n                       Other options: all-mpnet-base-v2 (better quality, slower)\n        """\n        self.model_name = model_name\n        try:\n            self.model = SentenceTransformer(model_name)\n            logger.info(f"Successfully loaded embedding model: {model_name}")\n        except Exception as e:\n            logger.error(f"Failed to load model {model_name}: {e}")\n            raise\n    \n    def embed_documents(self, texts: List[str], show_progress: bool = True) -> List[List[float]]:\n        """\n        Generate embeddings for a list of documents\n        \n        Args:\n            texts: List of text strings to embed\n            show_progress: Show progress bar during encoding\n        \n        Returns:\n            List of embedding vectors (each is a list of floats)\n        """\n        try:\n            embeddings = self.model.encode(\n                texts,\n                show_progress_bar=show_progress,\n                convert_to_numpy=True\n            )\n            logger.info(f"Generated embeddings for {len(texts)} documents")\n            return embeddings.tolist()\n        except Exception as e:\n            logger.error(f"Error generating document embeddings: {e}")\n            return []\n    \n    def embed_query(self, text: str) -> List[float]:\n        """\n        Generate embedding for a single query\n        \n        Args:\n            text: Query text to embed\n        \n        Returns:\n            Embedding vector as list of floats\n        """\n        try:\n            embedding = self.model.encode([text], convert_to_numpy=True)[0]\n            return embedding.tolist()\n        except Exception as e:\n            logger.error(f"Error generating query embedding: {e}")\n            return []\n    \n    def get_model_info(self) -> dict:\n        """\n        Get information about the current embedding model\n        \n        Returns:\n            Dictionary with model information\n        """\n        return {\n            "model_name": self.model_name,\n            "embedding_dimension": self.model.get_sentence_embedding_dimension(),\n            "max_seq_length": self.model.max_seq_length\n        }\n\n\nif __name__ == "__main__":\n    # Example usage\n    embedding_service = EmbeddingService()\n    \n    # Test with sample texts\n    texts = [\n        "What is the capital of India?",\n        "Delhi is the capital city of India",\n        "Python programming language"\n    ]\n    \n    # Generate embeddings\n    doc_embeddings = embedding_service.embed_documents(texts)\n    print(f"Generated {len(doc_embeddings)} embeddings")\n    \n    # Generate query embedding\n    query = "Capital of India"\n    query_embedding = embedding_service.embed_query(query)\n    print(f"Query embedding dimension: {len(query_embedding)}")\n    \n    # Show model info\n    info = embedding_service.get_model_info()\n    print(f"Model info: {info}")
