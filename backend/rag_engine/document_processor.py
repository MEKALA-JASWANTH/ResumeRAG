"""\nDocument Processor Module\nHandles extraction and processing of various document formats\n"""\n\nimport os\nfrom typing import List, Dict, Optional\nfrom pathlib import Path\nimport logging\n\n# Document processing\nfrom PyPDF2 import PdfReader\nfrom docx import Document\nimport pdfplumber\n\n# Text processing\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document as LangChainDocument\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass DocumentProcessor:\n    """\n    Process and extract text from various document formats\n    """\n    \n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 200\n    ):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            length_function=len,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n    def process_pdf(self, file_path: str) -> List[str]:\n        """\n        Extract text from PDF files\n        """\n        try:\n            text_content = []\n            \n            # Try with pdfplumber first (better for complex PDFs)\n            try:\n                with pdfplumber.open(file_path) as pdf:\n                    for page in pdf.pages:\n                        text = page.extract_text()\n                        if text:\n                            text_content.append(text)\n            except Exception as e:\n                logger.warning(f\"pdfplumber failed, trying PyPDF2: {e}\")\n                \n                # Fallback to PyPDF2\n                with open(file_path, 'rb') as file:\n                    pdf_reader = PdfReader(file)\n                    for page in pdf_reader.pages:\n                        text = page.extract_text()\n                        if text:\n                            text_content.append(text)\n            \n            logger.info(f\"Extracted {len(text_content)} pages from PDF\")\n            return text_content\n            \n        except Exception as e:\n            logger.error(f\"Error processing PDF {file_path}: {e}\")\n            return []\n    \n    def process_docx(self, file_path: str) -> List[str]:\n        """\n        Extract text from DOCX files\n        """\n        try:\n            doc = Document(file_path)\n            text_content = []\n            \n            for paragraph in doc.paragraphs:\n                if paragraph.text.strip():\n                    text_content.append(paragraph.text)\n            \n            logger.info(f\"Extracted {len(text_content)} paragraphs from DOCX\")\n            return text_content\n            \n        except Exception as e:\n            logger.error(f\"Error processing DOCX {file_path}: {e}\")\n            return []\n    \n    def process_txt(self, file_path: str) -> List[str]:\n        """\n        Extract text from TXT files\n        """\n        try:\n            with open(file_path, 'r', encoding='utf-8') as file:\n                content = file.read()\n                return [content]\n        except Exception as e:\n            logger.error(f\"Error processing TXT {file_path}: {e}\")\n            return []\n    \n    def process_document(self, file_path: str) -> List[str]:\n        """\n        Process document based on file extension\n        """\n        file_extension = Path(file_path).suffix.lower()\n        \n        if file_extension == '.pdf':\n            return self.process_pdf(file_path)\n        elif file_extension == '.docx':\n            return self.process_docx(file_path)\n        elif file_extension == '.txt':\n            return self.process_txt(file_path)\n        else:\n            logger.error(f\"Unsupported file format: {file_extension}\")\n            return []\n    \n    def chunk_text(self, text: str, metadata: Dict = None) -> List[LangChainDocument]:\n        """\n        Split text into chunks for processing\n        """\n        chunks = self.text_splitter.split_text(text)\n        \n        documents = [\n            LangChainDocument(\n                page_content=chunk,\n                metadata=metadata or {}\n            )\n            for chunk in chunks\n        ]\n        \n        logger.info(f\"Created {len(documents)} chunks from text\")\n        return documents\n    \n    def process_and_chunk(\n        self,\n        file_path: str,\n        metadata: Optional[Dict] = None\n    ) -> List[LangChainDocument]:\n        """\n        Process document and split into chunks\n        """\n        # Extract text\n        text_content = self.process_document(file_path)\n        \n        if not text_content:\n            return []\n        \n        # Combine all text\n        full_text = \"\\n\\n\".join(text_content)\n        \n        # Add file metadata\n        if metadata is None:\n            metadata = {}\n        \n        metadata.update({\n            \"source\": file_path,\n            \"filename\": Path(file_path).name,\n            \"file_type\": Path(file_path).suffix\n        })\n        \n        # Chunk the text\n        documents = self.chunk_text(full_text, metadata)\n        \n        return documents\n    \n    def process_multiple_documents(\n        self,\n        file_paths: List[str]\n    ) -> List[LangChainDocument]:\n        """\n        Process multiple documents and return all chunks\n        """\n        all_documents = []\n        \n        for file_path in file_paths:\n            logger.info(f\"Processing: {file_path}\")\n            documents = self.process_and_chunk(file_path)\n            all_documents.extend(documents)\n        \n        logger.info(f\"Processed {len(file_paths)} files, created {len(all_documents)} chunks\")\n        return all_documents\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    processor = DocumentProcessor()\n    \n    # Process a single document\n    docs = processor.process_and_chunk(\"sample.pdf\")\n    print(f\"Created {len(docs)} document chunks\")
